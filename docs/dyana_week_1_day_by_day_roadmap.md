# DYANA â€” Week 1 Day-by-Day Roadmap (Checklist)

âœ… done 
âŒ›running/slow 
ðŸš«paused 
â±ï¸needs review 
ðŸ”„in progress

**Start date:** Monday **2026-02-09**  
**End date:** Friday **2026-02-13**  
**Goal:** Build the *spine* of DYANA â€” a deterministic, end-to-end pipeline from audio to IPUs with metrics.

> Rule of the week: **no learning, no ASR, no iteration loops**. If itâ€™s not measurable, it doesnâ€™t belong here.

---

## Day 1 â€” Monday (2026-02-09)
### Canonical timebase & core contracts

**Objective:** Lock the global temporal backbone and data contracts. Everything depends on this.

### Concrete goals
- Define a single canonical time grid (10 ms hop)
- Implement resampling utilities
- Define and validate evidence data structures

### Checklist
- [âœ…] Define `TimeBase` (frame â†” time mapping)
- [âœ…] Enforce 10 ms global hop (`t = frame_idx * 0.01`)
- [âœ…] Implement upsampling (e.g. 20 ms â†’ 10 ms)
- [âœ…] Implement downsampling with explicit aggregation (max / mean)
- [âœ…] Implement `EvidenceTrack`
  - [âœ…] shape validation
  - [âœ…] explicit semantics (probability / score / logit)
  - [âœ…] optional confidence
- [âœ…] Implement `EvidenceBundle`
  - [âœ…] order-independent
  - [âœ…] missing tracks allowed
- [âœ…] Serialization to disk (NPZ / JSON)

**Exit criteria:**
- Any evidence track can be resampled onto the canonical grid
- Shape or timebase mismatch fails loudly

---

## Day 2 â€” Tuesday (2026-02-10)
### Axis 2 decoder skeleton (no real audio)

**Objective:** Build a stable structured decoder *before* touching real evidence.

### Concrete goals
- Implement minimal state space
- Implement Viterbi-style decoding
- Encode conversational constraints

### Checklist
- [âœ…] Define states: `SIL`, `A`, `B`, `OVL`, `LEAK`
- [âœ…] Implement framewise state score interface
- [âœ…] Implement Viterbi-style DP
- [âœ…] Add transition penalties:
  - [âœ…] state persistence cheap
  - [âœ…] speaker switch expensive
  - [âœ…] minimum IPU duration
  - [âœ…] minimum silence duration
- [âœ…] Enforce: **LEAK cannot initiate IPUs**
- [âœ…] Run decoder using fake / random evidence

**Exit criteria:**
- Decoder runs deterministically
- No Aâ†”B ping-pong
- No frame-level jitter

---

## Day 3 â€” Wednesday (2026-02-11)
### Axis 1: cheap, deterministic evidence

**Objective:** Plug in minimal, reliable evidence sources as independent modules.

### Concrete goals
- Extract trivial but robust speech cues
- Export all cues as independent EvidenceTracks

### Checklist
- [âœ…] Energy / RMS envelope per frame
- [âœ…] Smoothed energy envelope (50â€“100 ms)
- [âœ…] WebRTC VAD integration
  - [âœ…] exported as *soft* likelihood
  - [âœ…] never binary
- [âœ…] Minimal prosodic cues:
  - [âœ…] voiced / unvoiced flag
  - [âœ…] energy slope
- [âœ…] Each cue exported as its own EvidenceTrack
- [âœ…] Evidence caching enabled

**Exit criteria:**
- Removing any single evidence track does not crash decoding
- Evidence artifacts are inspectable on disk

---

## Day 4 â€” Thursday (2026-02-12)
### End-to-end pipeline & outputs

**Objective:** Make DYANA actually run from audio to annotation.

### Concrete goals
- Wire all components together
- Produce human-readable outputs

### Checklist
- [âœ…] Audio loading (mono + multi-channel)
- [âœ…] End-to-end execution:
  ```
  audio â†’ evidence bundle â†’ decoder â†’ state sequence
  ```
- [âœ…] IPU extraction from decoded states
- [âœ…] Praat TextGrid export:
  - [âœ…] Speaker A IPUs
  - [âœ…] Speaker B IPUs
  - [âœ…] Overlap
  - [âœ…] Leakage
- [âœ…] Dump intermediate artifacts (evidence + decoder outputs)

**Exit criteria:**
- Pipeline runs on multiple files without crashing
- Outputs are interpretable by inspection
- Results are identical across repeated runs

---

## Day 5 â€” Friday (2026-02-13)
### Evaluation harness & baseline scorecard

**Objective:** Ensure every future improvement is measurable.

### Concrete goals
- Implement core metrics
- Run evaluation on multiple dataset tiers

### Checklist
- [âœ…] Boundary F1 implementation
  - [âœ…] Â±20 ms tolerance
  - [âœ…] Â±50 ms tolerance
- [âœ…] Framewise IoU (speech vs silence, per speaker)
- [âœ…] Structural metrics:
  - [âœ…] micro-IPUs / min
  - [âœ…] speaker switches / min
  - [âœ…] rapid alternations
- [ ] Evaluation on:
  - [ ] synthetic / semi-synthetic data
  - [ ] easy real segment
  - [ ] hard real segment
- [ ] Baseline scorecard written to disk

**Exit criteria:**
- Metrics rerun identically across runs
- Removing evidence changes scores *predictably*

---

## End-of-Week Definition of Done

You may proceed to Week 2 **only if all are true**:

- [ ] Single canonical 10 ms timebase everywhere
- [ ] EvidenceTrack / EvidenceBundle used end-to-end
- [ ] Decoder stable with fake or missing evidence
- [ ] LEAK state blocks spurious IPUs
- [ ] TextGrid outputs look sane
- [ ] Metrics exist and are reproducible

> If any box is unchecked: fix it before adding complexity.

